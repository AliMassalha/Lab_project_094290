{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50497798-80d8-47d2-97d7-aed302f1c6eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"dbfs:/FileStore/shared_uploads/ali.m@campus.technion.ac.il/summaries_train.csv\" \n",
    "\n",
    "\n",
    "train = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"encoding\", \"UTF-8\")\\\n",
    "    .load(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3d8c77-c3fb-42c4-9f74-7ce1d4d8acde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: 926"
     ]
    }
   ],
   "source": [
    "train_sample = train.sample(False, 0.04, seed=85)\n",
    "\n",
    "train_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737fe3f8-382d-4f63-8442-679ff9dc3a43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\r\n",
      "  Using cached torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\r\n",
      "Collecting transformers\r\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\r\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\r\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\r\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from torch) (2.11.3)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\r\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.9/site-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.8.0)\r\n",
      "Collecting fsspec\r\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\r\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\r\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "Collecting nvidia-nccl-cu12==2.19.3\r\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\r\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "Collecting sympy\r\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "Collecting triton==2.2.0\r\n",
      "  Using cached triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "Collecting networkx\r\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\r\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\r\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\r\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12\r\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from transformers) (21.0)\r\n",
      "Collecting pyyaml>=5.1\r\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\r\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\r\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.9/site-packages (from transformers) (1.20.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.9/site-packages (from transformers) (2023.12.25)\r\n",
      "Collecting tokenizers<0.19,>=0.14\r\n",
      "  Using cached tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "Collecting safetensors>=0.4.1\r\n",
      "  Using cached safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.9/site-packages (from transformers) (4.66.2)\r\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from transformers) (2.26.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (3.2)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Collecting mpmath>=0.19\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Installing collected packages: pyyaml, nvidia-nvjitlink-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cublas-cu12, mpmath, huggingface-hub, triton, tokenizers, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, networkx, transformers, torch\r\n",
      "Successfully installed fsspec-2024.3.1 huggingface-hub-0.22.2 mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pyyaml-6.0.1 safetensors-0.4.2 sympy-1.12 tokenizers-0.15.2 torch-2.2.2 transformers-4.39.3 triton-2.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-0b757462-1e89-43b8-8a45-bb1136bde608/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92834d6d-2c40-44f4-bf49-fd135db46925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0d3bcc2f524df09b0ff9a9d98fa404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa5d1dfa5c4401e972d3b8da0bc6154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaa1ad473b64d6cb7c8c84e9f1106ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2c69674491493e81868038d3b60ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72572e7805c2419f86e6bec801985d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# df = pd.read_parquet('/path_to_local_storage/job_posts_prepared.parquet')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Tokenize data\n",
    "inputs = tokenizer(train_sample.rdd.map(lambda x: x[0]).collect(), \n",
    "                   max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d05efd-ede4-45d4-bb30-2972c0aee96c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c314e53a4d4711886dd9bfedc39a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eee4ff361b54ba19c1916a13f2f1bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class JobPostDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
    "\n",
    "dataset = JobPostDataset(inputs)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a033e183-5251-4542-b94e-8f5343bd9744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.9932637214660645\n",
      "Epoch: 0, Loss: 4.802980422973633\n",
      "Epoch: 0, Loss: 4.686226844787598\n",
      "Epoch: 0, Loss: 4.383626937866211\n",
      "Epoch: 0, Loss: 4.175486087799072\n",
      "Epoch: 0, Loss: 3.818389892578125\n",
      "Epoch: 0, Loss: 4.2484049797058105\n",
      "Epoch: 0, Loss: 4.03070592880249\n",
      "Epoch: 0, Loss: 4.260434627532959\n",
      "Epoch: 0, Loss: 3.865204334259033\n",
      "Epoch: 0, Loss: 3.6774380207061768\n",
      "Epoch: 0, Loss: 4.131802558898926\n",
      "Epoch: 0, Loss: 4.09811544418335\n",
      "Epoch: 0, Loss: 4.009454250335693\n",
      "Epoch: 0, Loss: 4.157166004180908\n",
      "Epoch: 0, Loss: 4.3007307052612305\n",
      "Epoch: 0, Loss: 4.0141754150390625\n",
      "Epoch: 0, Loss: 3.9246556758880615\n",
      "Epoch: 0, Loss: 3.981076240539551\n",
      "Epoch: 0, Loss: 3.9608993530273438\n",
      "Epoch: 0, Loss: 4.152264595031738\n",
      "Epoch: 0, Loss: 3.9835588932037354\n",
      "Epoch: 0, Loss: 3.6435391902923584\n",
      "Epoch: 0, Loss: 3.7954697608947754\n",
      "Epoch: 0, Loss: 3.9234023094177246\n",
      "Epoch: 0, Loss: 3.692347288131714\n",
      "Epoch: 0, Loss: 3.782850980758667\n",
      "Epoch: 0, Loss: 3.9707233905792236\n",
      "Epoch: 0, Loss: 4.017848491668701\n",
      "Epoch: 0, Loss: 3.940502882003784\n",
      "Epoch: 0, Loss: 3.653613328933716\n",
      "Epoch: 0, Loss: 3.497833013534546\n",
      "Epoch: 0, Loss: 3.7756471633911133\n",
      "Epoch: 0, Loss: 3.7124311923980713\n",
      "Epoch: 0, Loss: 3.8565595149993896\n",
      "Epoch: 0, Loss: 3.7943685054779053\n",
      "Epoch: 0, Loss: 3.5998620986938477\n",
      "Epoch: 0, Loss: 3.4557318687438965\n",
      "Epoch: 0, Loss: 3.7047295570373535\n",
      "Epoch: 0, Loss: 3.606158971786499\n",
      "Epoch: 0, Loss: 3.5238559246063232\n",
      "Epoch: 0, Loss: 3.604025363922119\n",
      "Epoch: 0, Loss: 3.8862361907958984\n",
      "Epoch: 0, Loss: 4.163955211639404\n",
      "Epoch: 0, Loss: 3.913278102874756\n",
      "Epoch: 0, Loss: 4.066621780395508\n",
      "Epoch: 0, Loss: 3.6272764205932617\n",
      "Epoch: 0, Loss: 3.9617245197296143\n",
      "Epoch: 0, Loss: 3.347299098968506\n",
      "Epoch: 0, Loss: 3.746628522872925\n",
      "Epoch: 0, Loss: 3.5981764793395996\n",
      "Epoch: 0, Loss: 3.891860008239746\n",
      "Epoch: 0, Loss: 3.51130747795105\n",
      "Epoch: 0, Loss: 3.5315001010894775\n",
      "Epoch: 0, Loss: 3.8783323764801025\n",
      "Epoch: 0, Loss: 3.6366331577301025\n",
      "Epoch: 0, Loss: 3.8626577854156494\n",
      "Epoch: 0, Loss: 3.5929927825927734\n",
      "Epoch: 0, Loss: 3.689940929412842\n",
      "Epoch: 0, Loss: 3.82651686668396\n",
      "Epoch: 0, Loss: 3.4027645587921143\n",
      "Epoch: 0, Loss: 3.8220810890197754\n",
      "Epoch: 0, Loss: 3.766263961791992\n",
      "Epoch: 0, Loss: 3.8686161041259766\n",
      "Epoch: 0, Loss: 3.800795316696167\n",
      "Epoch: 0, Loss: 3.908498525619507\n",
      "Epoch: 0, Loss: 3.6073708534240723\n",
      "Epoch: 0, Loss: 3.780839443206787\n",
      "Epoch: 0, Loss: 3.3215951919555664\n",
      "Epoch: 0, Loss: 3.674246311187744\n",
      "Epoch: 0, Loss: 3.764103651046753\n",
      "Epoch: 0, Loss: 3.6163244247436523\n",
      "Epoch: 0, Loss: 3.479654550552368\n",
      "Epoch: 0, Loss: 3.681727886199951\n",
      "Epoch: 0, Loss: 3.9239094257354736\n",
      "Epoch: 0, Loss: 3.599290370941162\n",
      "Epoch: 0, Loss: 3.8369274139404297\n",
      "Epoch: 0, Loss: 4.096318244934082\n",
      "Epoch: 0, Loss: 3.7757608890533447\n",
      "Epoch: 0, Loss: 3.834221601486206\n",
      "Epoch: 0, Loss: 3.8153374195098877\n",
      "Epoch: 0, Loss: 3.855365514755249\n",
      "Epoch: 0, Loss: 3.804302215576172\n",
      "Epoch: 0, Loss: 3.778367757797241\n",
      "Epoch: 0, Loss: 3.564314603805542\n",
      "Epoch: 0, Loss: 3.1611218452453613\n",
      "Epoch: 0, Loss: 3.4968509674072266\n",
      "Epoch: 0, Loss: 3.7709741592407227\n",
      "Epoch: 0, Loss: 4.105533123016357\n",
      "Epoch: 0, Loss: 3.3517589569091797\n",
      "Epoch: 0, Loss: 3.4853029251098633\n",
      "Epoch: 0, Loss: 3.665400981903076\n",
      "Epoch: 0, Loss: 3.6829006671905518\n",
      "Epoch: 0, Loss: 3.5572776794433594\n",
      "Epoch: 0, Loss: 3.834900379180908\n",
      "Epoch: 0, Loss: 3.284641981124878\n",
      "Epoch: 0, Loss: 3.45408296585083\n",
      "Epoch: 0, Loss: 4.089885711669922\n",
      "Epoch: 0, Loss: 3.7780866622924805\n",
      "Epoch: 0, Loss: 3.7719318866729736\n",
      "Epoch: 0, Loss: 3.614515781402588\n",
      "Epoch: 0, Loss: 3.448240041732788\n",
      "Epoch: 0, Loss: 3.6473472118377686\n",
      "Epoch: 0, Loss: 3.776538133621216\n",
      "Epoch: 0, Loss: 3.6054580211639404\n",
      "Epoch: 0, Loss: 3.751741647720337\n",
      "Epoch: 0, Loss: 3.837062120437622\n",
      "Epoch: 0, Loss: 3.593989849090576\n",
      "Epoch: 0, Loss: 3.8667490482330322\n",
      "Epoch: 0, Loss: 4.149302959442139\n",
      "Epoch: 0, Loss: 3.7236855030059814\n",
      "Epoch: 0, Loss: 3.8599045276641846\n",
      "Epoch: 0, Loss: 3.6543917655944824\n",
      "Epoch: 0, Loss: 3.9219419956207275\n",
      "Epoch: 0, Loss: 3.8065130710601807\n",
      "Epoch: 0, Loss: 3.4287171363830566\n",
      "Epoch: 0, Loss: 3.695336103439331\n",
      "Epoch: 0, Loss: 3.8666906356811523\n",
      "Epoch: 0, Loss: 3.5555026531219482\n",
      "Epoch: 0, Loss: 3.3823187351226807\n",
      "Epoch: 0, Loss: 3.7049190998077393\n",
      "Epoch: 0, Loss: 3.4165334701538086\n",
      "Epoch: 0, Loss: 3.1233768463134766\n",
      "Epoch: 0, Loss: 3.796415328979492\n",
      "Epoch: 0, Loss: 3.767791509628296\n",
      "Epoch: 0, Loss: 3.80967378616333\n",
      "Epoch: 0, Loss: 3.52919340133667\n",
      "Epoch: 0, Loss: 4.739904403686523\n",
      "Epoch: 0, Loss: 3.566035032272339\n",
      "Epoch: 0, Loss: 3.783323287963867\n",
      "Epoch: 0, Loss: 3.4756782054901123\n",
      "Epoch: 0, Loss: 3.528454303741455\n",
      "Epoch: 0, Loss: 3.6324896812438965\n",
      "Epoch: 0, Loss: 3.2829415798187256\n",
      "Epoch: 0, Loss: 3.7578868865966797\n",
      "Epoch: 0, Loss: 3.2711665630340576\n",
      "Epoch: 0, Loss: 3.6134400367736816\n",
      "Epoch: 0, Loss: 3.598558187484741\n",
      "Epoch: 0, Loss: 3.924484968185425\n",
      "Epoch: 0, Loss: 3.5304980278015137\n",
      "Epoch: 0, Loss: 4.327463626861572\n",
      "Epoch: 0, Loss: 3.7386012077331543\n",
      "Epoch: 0, Loss: 3.2673568725585938\n",
      "Epoch: 0, Loss: 3.6920597553253174\n",
      "Epoch: 0, Loss: 3.2831640243530273\n",
      "Epoch: 0, Loss: 3.618135452270508\n",
      "Epoch: 0, Loss: 3.7631449699401855\n",
      "Epoch: 0, Loss: 3.7436611652374268\n",
      "Epoch: 0, Loss: 3.5737123489379883\n",
      "Epoch: 0, Loss: 3.7334625720977783\n",
      "Epoch: 0, Loss: 3.9609248638153076\n",
      "Epoch: 0, Loss: 3.5546951293945312\n",
      "Epoch: 0, Loss: 3.5153603553771973\n",
      "Epoch: 0, Loss: 3.328857183456421\n",
      "Epoch: 0, Loss: 3.611755609512329\n",
      "Epoch: 0, Loss: 3.3589653968811035\n",
      "Epoch: 0, Loss: 3.4666569232940674\n",
      "Epoch: 0, Loss: 3.636502981185913\n",
      "Epoch: 0, Loss: 3.5685677528381348\n",
      "Epoch: 0, Loss: 3.734060525894165\n",
      "Epoch: 0, Loss: 3.038545846939087\n",
      "Epoch: 0, Loss: 3.810076951980591\n",
      "Epoch: 0, Loss: 3.436983108520508\n",
      "Epoch: 0, Loss: 3.665895700454712\n",
      "Epoch: 0, Loss: 3.7409934997558594\n",
      "Epoch: 0, Loss: 3.7433276176452637\n",
      "Epoch: 0, Loss: 3.574873208999634\n",
      "Epoch: 0, Loss: 3.3356680870056152\n",
      "Epoch: 0, Loss: 3.649230480194092\n",
      "Epoch: 0, Loss: 3.689040422439575\n",
      "Epoch: 0, Loss: 4.032822608947754\n",
      "Epoch: 0, Loss: 3.553454875946045\n",
      "Epoch: 0, Loss: 3.612091302871704\n",
      "Epoch: 0, Loss: 3.7286834716796875\n",
      "Epoch: 0, Loss: 3.772606134414673\n",
      "Epoch: 0, Loss: 3.555518388748169\n",
      "Epoch: 0, Loss: 3.63557505607605\n",
      "Epoch: 0, Loss: 3.534966230392456\n",
      "Epoch: 0, Loss: 2.963916063308716\n",
      "Epoch: 0, Loss: 3.706246852874756\n",
      "Epoch: 0, Loss: 3.7712771892547607\n",
      "Epoch: 0, Loss: 3.716700553894043\n",
      "Epoch: 0, Loss: 3.6328511238098145\n",
      "Epoch: 0, Loss: 3.750706434249878\n",
      "Epoch: 0, Loss: 3.32503342628479\n",
      "Epoch: 0, Loss: 3.740701675415039\n",
      "Epoch: 0, Loss: 3.784409523010254\n",
      "Epoch: 0, Loss: 3.657008171081543\n",
      "Epoch: 0, Loss: 3.767085075378418\n",
      "Epoch: 0, Loss: 3.7333061695098877\n",
      "Epoch: 0, Loss: 4.649003505706787\n",
      "Epoch: 0, Loss: 3.4159457683563232\n",
      "Epoch: 0, Loss: 3.855544090270996\n",
      "Epoch: 0, Loss: 3.524657726287842\n",
      "Epoch: 0, Loss: 3.814876079559326\n",
      "Epoch: 0, Loss: 3.528050422668457\n",
      "Epoch: 0, Loss: 3.690932035446167\n",
      "Epoch: 0, Loss: 3.65470552444458\n",
      "Epoch: 0, Loss: 3.690793752670288\n",
      "Epoch: 0, Loss: 3.4614415168762207\n",
      "Epoch: 0, Loss: 2.9082984924316406\n",
      "Epoch: 0, Loss: 3.4194211959838867\n",
      "Epoch: 0, Loss: 3.631546974182129\n",
      "Epoch: 0, Loss: 3.5987775325775146\n",
      "Epoch: 0, Loss: 3.550786018371582\n",
      "Epoch: 0, Loss: 3.519164562225342\n",
      "Epoch: 0, Loss: 2.521026372909546\n",
      "Epoch: 0, Loss: 3.345478057861328\n",
      "Epoch: 0, Loss: 3.832181930541992\n",
      "Epoch: 0, Loss: 3.8009531497955322\n",
      "Epoch: 0, Loss: 3.292142629623413\n",
      "Epoch: 0, Loss: 3.094059705734253\n",
      "Epoch: 0, Loss: 3.7765824794769287\n",
      "Epoch: 0, Loss: 3.0895144939422607\n",
      "Epoch: 0, Loss: 3.67142653465271\n",
      "Epoch: 0, Loss: 3.8601491451263428\n",
      "Epoch: 0, Loss: 3.2010657787323\n",
      "Epoch: 0, Loss: 3.649613380432129\n",
      "Epoch: 0, Loss: 3.383932113647461\n",
      "Epoch: 0, Loss: 3.8419342041015625\n",
      "Epoch: 0, Loss: 3.859895706176758\n",
      "Epoch: 0, Loss: 3.3070924282073975\n",
      "Epoch: 0, Loss: 3.520390748977661\n",
      "Epoch: 0, Loss: 3.810134172439575\n",
      "Epoch: 0, Loss: 3.564805030822754\n",
      "Epoch: 0, Loss: 3.3291075229644775\n",
      "Epoch: 0, Loss: 3.4792771339416504\n",
      "Epoch: 0, Loss: 3.891649007797241\n",
      "Epoch: 0, Loss: 2.838230609893799\n",
      "Epoch: 0, Loss: 3.6238694190979004\n",
      "Epoch: 0, Loss: 3.4845826625823975\n",
      "Epoch: 0, Loss: 3.935192108154297\n",
      "Epoch: 1, Loss: 3.2119603157043457\n",
      "Epoch: 1, Loss: 2.9922475814819336\n",
      "Epoch: 1, Loss: 3.1723549365997314\n",
      "Epoch: 1, Loss: 3.2395777702331543\n",
      "Epoch: 1, Loss: 3.19346022605896\n",
      "Epoch: 1, Loss: 3.3541674613952637\n",
      "Epoch: 1, Loss: 2.857875108718872\n",
      "Epoch: 1, Loss: 2.721613645553589\n",
      "Epoch: 1, Loss: 3.1106367111206055\n",
      "Epoch: 1, Loss: 3.065598726272583\n",
      "Epoch: 1, Loss: 2.894749402999878\n",
      "Epoch: 1, Loss: 2.8898468017578125\n",
      "Epoch: 1, Loss: 2.940885305404663\n",
      "Epoch: 1, Loss: 3.191498041152954\n",
      "Epoch: 1, Loss: 3.1352875232696533\n",
      "Epoch: 1, Loss: 2.6361584663391113\n",
      "Epoch: 1, Loss: 2.251955986022949\n",
      "Epoch: 1, Loss: 2.742297887802124\n",
      "Epoch: 1, Loss: 3.2929134368896484\n",
      "Epoch: 1, Loss: 3.277590274810791\n",
      "Epoch: 1, Loss: 3.3739945888519287\n",
      "Epoch: 1, Loss: 2.0649168491363525\n",
      "Epoch: 1, Loss: 3.1109471321105957\n",
      "Epoch: 1, Loss: 3.2561099529266357\n",
      "Epoch: 1, Loss: 2.968484878540039\n",
      "Epoch: 1, Loss: 2.505498170852661\n",
      "Epoch: 1, Loss: 2.6971640586853027\n",
      "Epoch: 1, Loss: 3.2566685676574707\n",
      "Epoch: 1, Loss: 3.0818567276000977\n",
      "Epoch: 1, Loss: 3.134798288345337\n",
      "Epoch: 1, Loss: 3.012371301651001\n",
      "Epoch: 1, Loss: 3.0769026279449463\n",
      "Epoch: 1, Loss: 2.8579797744750977\n",
      "Epoch: 1, Loss: 3.0595703125\n",
      "Epoch: 1, Loss: 3.412262439727783\n",
      "Epoch: 1, Loss: 3.1174702644348145\n",
      "Epoch: 1, Loss: 3.006965160369873\n",
      "Epoch: 1, Loss: 3.360957145690918\n",
      "Epoch: 1, Loss: 2.4417126178741455\n",
      "Epoch: 1, Loss: 3.4064643383026123\n",
      "Epoch: 1, Loss: 3.228837490081787\n",
      "Epoch: 1, Loss: 2.7531685829162598\n",
      "Epoch: 1, Loss: 2.905773162841797\n",
      "Epoch: 1, Loss: 2.6929261684417725\n",
      "Epoch: 1, Loss: 3.0422675609588623\n",
      "Epoch: 1, Loss: 2.925230026245117\n",
      "Epoch: 1, Loss: 3.3140058517456055\n",
      "Epoch: 1, Loss: 2.9913089275360107\n",
      "Epoch: 1, Loss: 2.192845582962036\n",
      "Epoch: 1, Loss: 3.07187819480896\n",
      "Epoch: 1, Loss: 3.105991840362549\n",
      "Epoch: 1, Loss: 3.0910778045654297\n",
      "Epoch: 1, Loss: 3.1460344791412354\n",
      "Epoch: 1, Loss: 3.314831018447876\n",
      "Epoch: 1, Loss: 3.3735110759735107\n",
      "Epoch: 1, Loss: 2.6282968521118164\n",
      "Epoch: 1, Loss: 2.960749864578247\n",
      "Epoch: 1, Loss: 3.2487967014312744\n",
      "Epoch: 1, Loss: 3.3923792839050293\n",
      "Epoch: 1, Loss: 3.21354603767395\n",
      "Epoch: 1, Loss: 3.2880187034606934\n",
      "Epoch: 1, Loss: 3.3970582485198975\n",
      "Epoch: 1, Loss: 2.7578964233398438\n",
      "Epoch: 1, Loss: 3.2065114974975586\n",
      "Epoch: 1, Loss: 2.694934129714966\n",
      "Epoch: 1, Loss: 3.2050909996032715\n",
      "Epoch: 1, Loss: 3.1356539726257324\n",
      "Epoch: 1, Loss: 3.282914161682129\n",
      "Epoch: 1, Loss: 2.5526201725006104\n",
      "Epoch: 1, Loss: 3.531221866607666\n",
      "Epoch: 1, Loss: 2.7286252975463867\n",
      "Epoch: 1, Loss: 2.8348264694213867\n",
      "Epoch: 1, Loss: 2.6957383155822754\n",
      "Epoch: 1, Loss: 3.0967369079589844\n",
      "Epoch: 1, Loss: 2.9946463108062744\n",
      "Epoch: 1, Loss: 3.2310729026794434\n",
      "Epoch: 1, Loss: 3.348616123199463\n",
      "Epoch: 1, Loss: 2.9975225925445557\n",
      "Epoch: 1, Loss: 3.3171186447143555\n",
      "Epoch: 1, Loss: 3.1911308765411377\n",
      "Epoch: 1, Loss: 3.0285491943359375\n",
      "Epoch: 1, Loss: 2.8277602195739746\n",
      "Epoch: 1, Loss: 3.1093077659606934\n",
      "Epoch: 1, Loss: 3.1283411979675293\n",
      "Epoch: 1, Loss: 3.3974051475524902\n",
      "Epoch: 1, Loss: 3.2153759002685547\n",
      "Epoch: 1, Loss: 3.103689193725586\n",
      "Epoch: 1, Loss: 3.2743852138519287\n",
      "Epoch: 1, Loss: 3.2253332138061523\n",
      "Epoch: 1, Loss: 3.318682909011841\n",
      "Epoch: 1, Loss: 3.7015933990478516\n",
      "Epoch: 1, Loss: 3.188641309738159\n",
      "Epoch: 1, Loss: 2.7169034481048584\n",
      "Epoch: 1, Loss: 2.9417223930358887\n",
      "Epoch: 1, Loss: 2.9446959495544434\n",
      "Epoch: 1, Loss: 3.0608701705932617\n",
      "Epoch: 1, Loss: 3.2729454040527344\n",
      "Epoch: 1, Loss: 2.971996545791626\n",
      "Epoch: 1, Loss: 2.8966801166534424\n",
      "Epoch: 1, Loss: 3.3381261825561523\n",
      "Epoch: 1, Loss: 3.3445093631744385\n",
      "Epoch: 1, Loss: 3.165437936782837\n",
      "Epoch: 1, Loss: 3.270223379135132\n",
      "Epoch: 1, Loss: 3.3627288341522217\n",
      "Epoch: 1, Loss: 3.4492685794830322\n",
      "Epoch: 1, Loss: 2.760014772415161\n",
      "Epoch: 1, Loss: 2.9248831272125244\n",
      "Epoch: 1, Loss: 3.176997423171997\n",
      "Epoch: 1, Loss: 3.3091607093811035\n",
      "Epoch: 1, Loss: 3.0135505199432373\n",
      "Epoch: 1, Loss: 2.5724780559539795\n",
      "Epoch: 1, Loss: 3.191066265106201\n",
      "Epoch: 1, Loss: 2.970289707183838\n",
      "Epoch: 1, Loss: 3.310915470123291\n",
      "Epoch: 1, Loss: 2.586510181427002\n",
      "Epoch: 1, Loss: 3.1926889419555664\n",
      "Epoch: 1, Loss: 3.1366994380950928\n",
      "Epoch: 1, Loss: 2.5128204822540283\n",
      "Epoch: 1, Loss: 3.1080482006073\n",
      "Epoch: 1, Loss: 2.650683641433716\n",
      "Epoch: 1, Loss: 3.275056838989258\n",
      "Epoch: 1, Loss: 3.0024938583374023\n",
      "Epoch: 1, Loss: 3.1343913078308105\n",
      "Epoch: 1, Loss: 2.8676278591156006\n",
      "Epoch: 1, Loss: 3.036080837249756\n",
      "Epoch: 1, Loss: 3.1074321269989014\n",
      "Epoch: 1, Loss: 3.1897523403167725\n",
      "Epoch: 1, Loss: 3.292160987854004\n",
      "Epoch: 1, Loss: 3.1335439682006836\n",
      "Epoch: 1, Loss: 2.516489267349243\n",
      "Epoch: 1, Loss: 2.5846500396728516\n",
      "Epoch: 1, Loss: 2.8703393936157227\n",
      "Epoch: 1, Loss: 2.9911530017852783\n",
      "Epoch: 1, Loss: 2.4781527519226074\n",
      "Epoch: 1, Loss: 3.1135239601135254\n",
      "Epoch: 1, Loss: 3.1420655250549316\n",
      "Epoch: 1, Loss: 3.183798313140869\n",
      "Epoch: 1, Loss: 2.912349224090576\n",
      "Epoch: 1, Loss: 2.7871029376983643\n",
      "Epoch: 1, Loss: 3.1097912788391113\n",
      "Epoch: 1, Loss: 3.157095432281494\n",
      "Epoch: 1, Loss: 3.2346489429473877\n",
      "Epoch: 1, Loss: 3.007114887237549\n",
      "Epoch: 1, Loss: 2.4703519344329834\n",
      "Epoch: 1, Loss: 3.213711977005005\n",
      "Epoch: 1, Loss: 2.7073683738708496\n",
      "Epoch: 1, Loss: 2.8152718544006348\n",
      "Epoch: 1, Loss: 2.6753242015838623\n",
      "Epoch: 1, Loss: 2.8720593452453613\n",
      "Epoch: 1, Loss: 2.8049545288085938\n",
      "Epoch: 1, Loss: 3.0836212635040283\n",
      "Epoch: 1, Loss: 3.1363682746887207\n",
      "Epoch: 1, Loss: 3.401508092880249\n",
      "Epoch: 1, Loss: 3.1903767585754395\n",
      "Epoch: 1, Loss: 3.0512657165527344\n",
      "Epoch: 1, Loss: 3.363835334777832\n",
      "Epoch: 1, Loss: 2.8048255443573\n",
      "Epoch: 1, Loss: 2.9029476642608643\n",
      "Epoch: 1, Loss: 2.705164670944214\n",
      "Epoch: 1, Loss: 2.8908324241638184\n",
      "Epoch: 1, Loss: 2.594151735305786\n",
      "Epoch: 1, Loss: 3.2746684551239014\n",
      "Epoch: 1, Loss: 2.7333946228027344\n",
      "Epoch: 1, Loss: 3.035879373550415\n",
      "Epoch: 1, Loss: 3.599698066711426\n",
      "Epoch: 1, Loss: 2.8080894947052\n",
      "Epoch: 1, Loss: 2.963139533996582\n",
      "Epoch: 1, Loss: 3.3287744522094727\n",
      "Epoch: 1, Loss: 2.8349313735961914\n",
      "Epoch: 1, Loss: 3.0350253582000732\n",
      "Epoch: 1, Loss: 2.5552899837493896\n",
      "Epoch: 1, Loss: 3.292567729949951\n",
      "Epoch: 1, Loss: 3.357748031616211\n",
      "Epoch: 1, Loss: 2.980062246322632\n",
      "Epoch: 1, Loss: 3.3196823596954346\n",
      "Epoch: 1, Loss: 3.1905579566955566\n",
      "Epoch: 1, Loss: 3.338536262512207\n",
      "Epoch: 1, Loss: 3.3777003288269043\n",
      "Epoch: 1, Loss: 3.274489402770996\n",
      "Epoch: 1, Loss: 1.8899415731430054\n",
      "Epoch: 1, Loss: 2.363670587539673\n",
      "Epoch: 1, Loss: 3.0049502849578857\n",
      "Epoch: 1, Loss: 3.2593371868133545\n",
      "Epoch: 1, Loss: 2.9435837268829346\n",
      "Epoch: 1, Loss: 2.8092901706695557\n",
      "Epoch: 1, Loss: 3.3899471759796143\n",
      "Epoch: 1, Loss: 3.00294828414917\n",
      "Epoch: 1, Loss: 3.293124198913574\n",
      "Epoch: 1, Loss: 3.4242827892303467\n",
      "Epoch: 1, Loss: 2.9333510398864746\n",
      "Epoch: 1, Loss: 3.2592530250549316\n",
      "Epoch: 1, Loss: 3.368022918701172\n",
      "Epoch: 1, Loss: 3.339240074157715\n",
      "Epoch: 1, Loss: 3.2136824131011963\n",
      "Epoch: 1, Loss: 3.329144239425659\n",
      "Epoch: 1, Loss: 3.6921708583831787\n",
      "Epoch: 1, Loss: 2.8910584449768066\n",
      "Epoch: 1, Loss: 2.99072527885437\n",
      "Epoch: 1, Loss: 3.114061117172241\n",
      "Epoch: 1, Loss: 3.3603014945983887\n",
      "Epoch: 1, Loss: 3.1662466526031494\n",
      "Epoch: 1, Loss: 3.2036256790161133\n",
      "Epoch: 1, Loss: 2.9875941276550293\n",
      "Epoch: 1, Loss: 2.467970848083496\n",
      "Epoch: 1, Loss: 2.9572536945343018\n",
      "Epoch: 1, Loss: 2.9246294498443604\n",
      "Epoch: 1, Loss: 3.125697135925293\n",
      "Epoch: 1, Loss: 3.901970148086548\n",
      "Epoch: 1, Loss: 3.269317626953125\n",
      "Epoch: 1, Loss: 3.1319477558135986\n",
      "Epoch: 1, Loss: 3.519913911819458\n",
      "Epoch: 1, Loss: 2.4018025398254395\n",
      "Epoch: 1, Loss: 2.684959888458252\n",
      "Epoch: 1, Loss: 3.0069432258605957\n",
      "Epoch: 1, Loss: 3.2842257022857666\n",
      "Epoch: 1, Loss: 2.8752081394195557\n",
      "Epoch: 1, Loss: 2.2292346954345703\n",
      "Epoch: 1, Loss: 3.096982717514038\n",
      "Epoch: 1, Loss: 3.3557026386260986\n",
      "Epoch: 1, Loss: 3.192253351211548\n",
      "Epoch: 1, Loss: 3.3604273796081543\n",
      "Epoch: 1, Loss: 3.3190791606903076\n",
      "Epoch: 1, Loss: 3.4310154914855957\n",
      "Epoch: 1, Loss: 3.2711338996887207\n",
      "Epoch: 1, Loss: 3.3618860244750977\n",
      "Epoch: 1, Loss: 2.6252143383026123\n",
      "Epoch: 1, Loss: 3.173884868621826\n",
      "Epoch: 1, Loss: 3.0010557174682617\n",
      "Epoch: 1, Loss: 3.2181596755981445\n",
      "Epoch: 1, Loss: 3.213552951812744\n",
      "Epoch: 1, Loss: 3.149167537689209\n",
      "Epoch: 1, Loss: 3.1731297969818115\n",
      "Epoch: 2, Loss: 2.8608553409576416\n",
      "Epoch: 2, Loss: 2.768467426300049\n",
      "Epoch: 2, Loss: 2.772442579269409\n",
      "Epoch: 2, Loss: 2.324089527130127\n",
      "Epoch: 2, Loss: 2.1845479011535645\n",
      "Epoch: 2, Loss: 2.7964401245117188\n",
      "Epoch: 2, Loss: 2.7474935054779053\n",
      "Epoch: 2, Loss: 2.5159385204315186\n",
      "Epoch: 2, Loss: 2.1779260635375977\n",
      "Epoch: 2, Loss: 2.772813081741333\n",
      "Epoch: 2, Loss: 2.5177478790283203\n",
      "Epoch: 2, Loss: 2.469252824783325\n",
      "Epoch: 2, Loss: 2.2161905765533447\n",
      "Epoch: 2, Loss: 2.3589818477630615\n",
      "Epoch: 2, Loss: 2.7847461700439453\n",
      "Epoch: 2, Loss: 2.716917037963867\n",
      "Epoch: 2, Loss: 2.156996488571167\n",
      "Epoch: 2, Loss: 2.432514190673828\n",
      "Epoch: 2, Loss: 2.9990487098693848\n",
      "Epoch: 2, Loss: 2.7688169479370117\n",
      "Epoch: 2, Loss: 2.5706140995025635\n",
      "Epoch: 2, Loss: 2.8608510494232178\n",
      "Epoch: 2, Loss: 2.582517385482788\n",
      "Epoch: 2, Loss: 2.7800607681274414\n",
      "Epoch: 2, Loss: 2.7029671669006348\n",
      "Epoch: 2, Loss: 2.301547050476074\n",
      "Epoch: 2, Loss: 2.7919349670410156\n",
      "Epoch: 2, Loss: 2.4174273014068604\n",
      "Epoch: 2, Loss: 2.864417791366577\n",
      "Epoch: 2, Loss: 2.5199978351593018\n",
      "Epoch: 2, Loss: 2.474881172180176\n",
      "Epoch: 2, Loss: 2.4560422897338867\n",
      "Epoch: 2, Loss: 2.391310930252075\n",
      "Epoch: 2, Loss: 2.429831027984619\n",
      "Epoch: 2, Loss: 2.9173038005828857\n",
      "Epoch: 2, Loss: 2.7250823974609375\n",
      "Epoch: 2, Loss: 3.0162465572357178\n",
      "Epoch: 2, Loss: 2.5137851238250732\n",
      "Epoch: 2, Loss: 3.035820722579956\n",
      "Epoch: 2, Loss: 2.8462257385253906\n",
      "Epoch: 2, Loss: 2.925933361053467\n",
      "Epoch: 2, Loss: 2.867094039916992\n",
      "Epoch: 2, Loss: 2.649482488632202\n",
      "Epoch: 2, Loss: 1.8247240781784058\n",
      "Epoch: 2, Loss: 3.0158660411834717\n",
      "Epoch: 2, Loss: 2.8329625129699707\n",
      "Epoch: 2, Loss: 2.6231446266174316\n",
      "Epoch: 2, Loss: 3.0696120262145996\n",
      "Epoch: 2, Loss: 2.5487215518951416\n",
      "Epoch: 2, Loss: 3.0032622814178467\n",
      "Epoch: 2, Loss: 2.752938747406006\n",
      "Epoch: 2, Loss: 2.854715585708618\n",
      "Epoch: 2, Loss: 2.2453010082244873\n",
      "Epoch: 2, Loss: 2.6116766929626465\n",
      "Epoch: 2, Loss: 3.0502216815948486\n",
      "Epoch: 2, Loss: 2.109523057937622\n",
      "Epoch: 2, Loss: 2.867302417755127\n",
      "Epoch: 2, Loss: 2.4184975624084473\n",
      "Epoch: 2, Loss: 2.3536133766174316\n",
      "Epoch: 2, Loss: 3.0416762828826904\n",
      "Epoch: 2, Loss: 2.565747022628784\n",
      "Epoch: 2, Loss: 2.2935831546783447\n",
      "Epoch: 2, Loss: 2.2236557006835938\n",
      "Epoch: 2, Loss: 2.485192060470581\n",
      "Epoch: 2, Loss: 2.992990016937256\n",
      "Epoch: 2, Loss: 1.7864525318145752\n",
      "Epoch: 2, Loss: 1.9470456838607788\n",
      "Epoch: 2, Loss: 2.7225341796875\n",
      "Epoch: 2, Loss: 2.7844574451446533\n",
      "Epoch: 2, Loss: 2.8403749465942383\n",
      "Epoch: 2, Loss: 2.453381061553955\n",
      "Epoch: 2, Loss: 1.7809391021728516\n",
      "Epoch: 2, Loss: 2.8215572834014893\n",
      "Epoch: 2, Loss: 2.4254961013793945\n",
      "Epoch: 2, Loss: 2.891610622406006\n",
      "Epoch: 2, Loss: 2.4873030185699463\n",
      "Epoch: 2, Loss: 1.8744386434555054\n",
      "Epoch: 2, Loss: 2.7596182823181152\n",
      "Epoch: 2, Loss: 3.000422239303589\n",
      "Epoch: 2, Loss: 2.865633726119995\n",
      "Epoch: 2, Loss: 2.3669235706329346\n",
      "Epoch: 2, Loss: 2.0661776065826416\n",
      "Epoch: 2, Loss: 2.815176486968994\n",
      "Epoch: 2, Loss: 2.725172996520996\n",
      "Epoch: 2, Loss: 2.766218900680542\n",
      "Epoch: 2, Loss: 2.6945412158966064\n",
      "Epoch: 2, Loss: 2.9941487312316895\n",
      "Epoch: 2, Loss: 2.7633628845214844\n",
      "Epoch: 2, Loss: 2.759756326675415\n",
      "Epoch: 2, Loss: 2.8353588581085205\n",
      "Epoch: 2, Loss: 2.8870840072631836\n",
      "Epoch: 2, Loss: 2.609389543533325\n",
      "Epoch: 2, Loss: 2.131844997406006\n",
      "Epoch: 2, Loss: 2.4519917964935303\n",
      "Epoch: 2, Loss: 2.9596614837646484\n",
      "Epoch: 2, Loss: 2.7978522777557373\n",
      "Epoch: 2, Loss: 2.9079034328460693\n",
      "Epoch: 2, Loss: 2.5525717735290527\n",
      "Epoch: 2, Loss: 2.8396859169006348\n",
      "Epoch: 2, Loss: 3.037391185760498\n",
      "Epoch: 2, Loss: 2.7525758743286133\n",
      "Epoch: 2, Loss: 1.7364652156829834\n",
      "Epoch: 2, Loss: 2.503257989883423\n",
      "Epoch: 2, Loss: 2.816118001937866\n",
      "Epoch: 2, Loss: 2.3913991451263428\n",
      "Epoch: 2, Loss: 2.8619441986083984\n",
      "Epoch: 2, Loss: 3.1104297637939453\n",
      "Epoch: 2, Loss: 2.385991334915161\n",
      "Epoch: 2, Loss: 2.7595043182373047\n",
      "Epoch: 2, Loss: 1.8011726140975952\n",
      "Epoch: 2, Loss: 2.867628335952759\n",
      "Epoch: 2, Loss: 2.4467530250549316\n",
      "Epoch: 2, Loss: 2.8352482318878174\n",
      "Epoch: 2, Loss: 2.56191086769104\n",
      "Epoch: 2, Loss: 2.7157442569732666\n",
      "Epoch: 2, Loss: 2.6797592639923096\n",
      "Epoch: 2, Loss: 2.826779365539551\n",
      "Epoch: 2, Loss: 2.1136953830718994\n",
      "Epoch: 2, Loss: 2.199040651321411\n",
      "Epoch: 2, Loss: 2.5351345539093018\n",
      "Epoch: 2, Loss: 2.3938193321228027\n",
      "Epoch: 2, Loss: 2.8181586265563965\n",
      "Epoch: 2, Loss: 3.0893778800964355\n",
      "Epoch: 2, Loss: 2.8507652282714844\n",
      "Epoch: 2, Loss: 2.489408016204834\n",
      "Epoch: 2, Loss: 2.796261787414551\n",
      "Epoch: 2, Loss: 2.3382604122161865\n",
      "Epoch: 2, Loss: 2.8938169479370117\n",
      "Epoch: 2, Loss: 2.534787893295288\n",
      "Epoch: 2, Loss: 2.8777241706848145\n",
      "Epoch: 2, Loss: 2.657597541809082\n",
      "Epoch: 2, Loss: 2.833876132965088\n",
      "Epoch: 2, Loss: 2.7746834754943848\n",
      "Epoch: 2, Loss: 2.840308427810669\n",
      "Epoch: 2, Loss: 2.606703519821167\n",
      "Epoch: 2, Loss: 2.597979784011841\n",
      "Epoch: 2, Loss: 2.0088024139404297\n",
      "Epoch: 2, Loss: 2.731781482696533\n",
      "Epoch: 2, Loss: 2.8198423385620117\n",
      "Epoch: 2, Loss: 2.0779850482940674\n",
      "Epoch: 2, Loss: 2.0045242309570312\n",
      "Epoch: 2, Loss: 2.7494430541992188\n",
      "Epoch: 2, Loss: 2.8813512325286865\n",
      "Epoch: 2, Loss: 1.817982792854309\n",
      "Epoch: 2, Loss: 2.1838901042938232\n",
      "Epoch: 2, Loss: 2.6190664768218994\n",
      "Epoch: 2, Loss: 2.912390947341919\n",
      "Epoch: 2, Loss: 2.8526861667633057\n",
      "Epoch: 2, Loss: 3.0206823348999023\n",
      "Epoch: 2, Loss: 1.9942227602005005\n",
      "Epoch: 2, Loss: 2.807447910308838\n",
      "Epoch: 2, Loss: 2.991156816482544\n",
      "Epoch: 2, Loss: 2.3906805515289307\n",
      "Epoch: 2, Loss: 2.666412591934204\n",
      "Epoch: 2, Loss: 2.6226985454559326\n",
      "Epoch: 2, Loss: 2.2763779163360596\n",
      "Epoch: 2, Loss: 2.4285809993743896\n",
      "Epoch: 2, Loss: 2.901865243911743\n",
      "Epoch: 2, Loss: 2.7497873306274414\n",
      "Epoch: 2, Loss: 2.951186180114746\n",
      "Epoch: 2, Loss: 2.367525100708008\n",
      "Epoch: 2, Loss: 2.460930585861206\n",
      "Epoch: 2, Loss: 2.4576759338378906\n",
      "Epoch: 2, Loss: 2.765467643737793\n",
      "Epoch: 2, Loss: 2.527921199798584\n",
      "Epoch: 2, Loss: 2.349360704421997\n",
      "Epoch: 2, Loss: 2.618553638458252\n",
      "Epoch: 2, Loss: 2.945495843887329\n",
      "Epoch: 2, Loss: 2.8491146564483643\n",
      "Epoch: 2, Loss: 2.9605071544647217\n",
      "Epoch: 2, Loss: 2.9472436904907227\n",
      "Epoch: 2, Loss: 2.396906852722168\n",
      "Epoch: 2, Loss: 2.9986536502838135\n",
      "Epoch: 2, Loss: 2.7524349689483643\n",
      "Epoch: 2, Loss: 1.9948310852050781\n",
      "Epoch: 2, Loss: 2.9412992000579834\n",
      "Epoch: 2, Loss: 2.4957752227783203\n",
      "Epoch: 2, Loss: 2.8887228965759277\n",
      "Epoch: 2, Loss: 2.868842840194702\n",
      "Epoch: 2, Loss: 2.5222201347351074\n",
      "Epoch: 2, Loss: 2.8567399978637695\n",
      "Epoch: 2, Loss: 2.2078404426574707\n",
      "Epoch: 2, Loss: 2.994267225265503\n",
      "Epoch: 2, Loss: 3.0178849697113037\n",
      "Epoch: 2, Loss: 2.5610358715057373\n",
      "Epoch: 2, Loss: 2.7889766693115234\n",
      "Epoch: 2, Loss: 1.787161111831665\n",
      "Epoch: 2, Loss: 3.074547529220581\n",
      "Epoch: 2, Loss: 2.335425853729248\n",
      "Epoch: 2, Loss: 3.031209707260132\n",
      "Epoch: 2, Loss: 3.0166707038879395\n",
      "Epoch: 2, Loss: 1.1514865159988403\n",
      "Epoch: 2, Loss: 2.560453176498413\n",
      "Epoch: 2, Loss: 2.661480188369751\n",
      "Epoch: 2, Loss: 3.030000686645508\n",
      "Epoch: 2, Loss: 2.466369867324829\n",
      "Epoch: 2, Loss: 2.984480142593384\n",
      "Epoch: 2, Loss: 2.8913633823394775\n",
      "Epoch: 2, Loss: 2.5584652423858643\n",
      "Epoch: 2, Loss: 2.9426541328430176\n",
      "Epoch: 2, Loss: 3.1365914344787598\n",
      "Epoch: 2, Loss: 2.8683598041534424\n",
      "Epoch: 2, Loss: 2.781724214553833\n",
      "Epoch: 2, Loss: 2.456662178039551\n",
      "Epoch: 2, Loss: 3.079897880554199\n",
      "Epoch: 2, Loss: 2.9533071517944336\n",
      "Epoch: 2, Loss: 2.659424066543579\n",
      "Epoch: 2, Loss: 2.204343795776367\n",
      "Epoch: 2, Loss: 2.4829142093658447\n",
      "Epoch: 2, Loss: 2.6634151935577393\n",
      "Epoch: 2, Loss: 3.0791611671447754\n",
      "Epoch: 2, Loss: 2.5984442234039307\n",
      "Epoch: 2, Loss: 2.9714515209198\n",
      "Epoch: 2, Loss: 2.8546111583709717\n",
      "Epoch: 2, Loss: 2.482208013534546\n",
      "Epoch: 2, Loss: 2.609999179840088\n",
      "Epoch: 2, Loss: 2.9807660579681396\n",
      "Epoch: 2, Loss: 2.238107681274414\n",
      "Epoch: 2, Loss: 2.8298001289367676\n",
      "Epoch: 2, Loss: 2.6720032691955566\n",
      "Epoch: 2, Loss: 2.54164457321167\n",
      "Epoch: 2, Loss: 2.9788978099823\n",
      "Epoch: 2, Loss: 2.7125227451324463\n",
      "Epoch: 2, Loss: 3.042952299118042\n",
      "Epoch: 2, Loss: 2.490450143814087\n",
      "Epoch: 2, Loss: 2.9270212650299072\n",
      "Epoch: 2, Loss: 2.8599274158477783\n",
      "Epoch: 2, Loss: 2.897993564605713\n",
      "Epoch: 2, Loss: 2.700563907623291\n",
      "Epoch: 2, Loss: 2.409073829650879\n",
      "Epoch: 2, Loss: 2.900632619857788\n",
      "Epoch: 2, Loss: 2.9398505687713623\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for epoch in range(3):  # number of epochs\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['input_ids'].to(device)  # GPT uses the same input as labels for language modeling\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a595ae-e2b9-4a4e-aa93-3711f3552e88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('dbfs:/FileStore/shared_uploads/ali.m@campus.technion.ac.il/jobs_gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5fc1d3-57b5-440e-bb4a-1a4e44dadc28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-0b757462-1e89-43b8-8a45-bb1136bde608/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Technical requirements for the software developer position include but are not limited to:\n",
      "We support a critical mission that is changing massively parallelism. We create an automated and lightning-fast internet, which provides hardware and networking infrastructure in all of our major business functions such as TCP/UDP routers or routers; we require users, engineers know how they can work across multiple systems at scale, both on one project, by defining and with a single effort.\n",
      "This requires frequent tasks that typically involve either running upshift – even in tight deadlines (or potentially involving close – because no other team would be more than perfect). Our mission has been driven through a determination to make its impact internationally. The company prides itself upon leading high quality standards, upholding core principles while being well developed and building trusted products for our customers—for adding value wherever possible! To truly stand out here without compromise the status quo, keep us rooted into everything it was like you could ever imagine. And this must continue to inspire your teams around where they’re important to them. If there s something interesting and hardy that keeps you ahead in any future, feel confident writing better every time - read about... https://t.co//u8d5ovxhzSjEJqYp4DmoBvLw0rXc?\n",
      "You will be responsible* *How You Will Help Us Make An Impact In Engineering Projects** As Possible By Working With This Team Role Or Someone Else Who Can Help Them That Will Benefit From Being Technologyobsessed Researchers\n",
      "Working independently within Comput-Centered Systems engineering practices does take off some responsibility from learning big ideas into designing/learning big data models or real algorithms. It's tough to think critically outside their box yet rather actively navigating through technical fields whether designing or building deep understanding concepts inside code means putting your clients first using emerging technologies when tackling complex challenges before they use cases. Using your background working closely together and communicating what matters between a variety and internal networks isn't typical if you're searching open to solving problems efficiently enough anywhere else you'll need help to get those obstacles away. Your responsibilities may vary based primarily locally so we have minimal remote oversight available under the applicable contract terms regardlessof who you are connecting client requests shouldering assignments.- At times these new developments aren'ar are rare since most projects were never designed prioritizes existing design efforts once it arrived including some combination training, mentoring, motivating testing / development over short periods required depending On What Matters With These Examples Of Projects Are Typically Constructive\n"
     ]
    }
   ],
   "source": [
    "# max_length: The maximum length of the sequence to be generated.\n",
    "# num_return_sequences: Number of sequences to return.\n",
    "# no_repeat_ngram_size: Ensures no n-gram appears twice.\n",
    "# repetition_penalty: The penalty for repeating the same line. Higher is more strict.\n",
    "# top_p: Controls diversity via nucleus sampling: top p tokens are considered.\n",
    "# temperature: The higher the temperature, the more random the output. Closer to zero makes it more deterministic.\n",
    "# do_sample: Whether or not to use sampling; use True for diverse generation.\n",
    "# top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "# early_stopping: Whether to stop the beam search when at least num_beams sentences are finished per batch.\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    # Encode the input prompt to get the token ids\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=2.5,\n",
    "        top_p=0.92,\n",
    "        temperature=0.85,\n",
    "        do_sample=True,\n",
    "        top_k=125,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode and print the output text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Technical requirements for the software developer position include\"\n",
    "generate_text(prompt, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0547df2c-4762-4a8d-b0ab-e52a3e95d629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-0b757462-1e89-43b8-8a45-bb1136bde608/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "write a job posting for sales representative in the market\n",
      "of DEVA (Excel: Retail, General & Military) and International?\n",
      "If you’re looking to make an immediate impact on our customers' lives while using your strong analytical skills as well taking pride of what matters most - if not exceeding their expectations. If that sounds like you, we want YOU TO BRING out with us! We hire motivated individuals with purpose-driven problem solving skills who share values through relentless ambition or unwavering belief system integrity – be yourself! With a track record supporting all people, let's take ownership where it comes from anywhere! As leader at H2COO I believe this is why every person deserves recognition regardless of their role being owner owned by another company.\n",
      "Join one Of America′s Top Workplaces® Where You Are Working For Our People And Do What Is That Look Like To Us? In The Role OF Your Key Accountant Coach Someone. This Managers Are Responsible And Excited Because They Understand Their Teamwork And Drive Results; These Guardians Can Help Them Be Our Greatest Asset. But Some Others Just… Thrive Each Other No Two Days Later When You Need It. Whatever Or Every Day Else Can't Come From Anyplace On TV. A Typical Leader Responds Personally rather than others are his/her team members having fun along side when needed so they can meet their business goals instead Instead Having Fun Over Time etc., Play Here Specialty Clips Req ills Or Casual... Good Jobs Allied Paid Holidays Paid Vacation days paid sick leave plus company holidays pay up until proven success on personal projects such e.. But don “t feel afraid about working here** because even before long ago, there was never really got any other thing planned for me... but now.... There had always been something special behind the scenes right around....well just yesterday after 9/11 #11 crashdump... DRIFT CARNORPHONE MANAGEMENT PROGRAMS FOR KILLED PEOPLE AND WHAT IS OUR GUESTICE? Well then read across your lines bringing together more into service today!! *Displaying humility based approach* Regardless whether the work you do isnHeartworish Owning one brand new product has gained momentum over recent years, iA Better Known how good Yatch looks looked could get adopted tomorrow : http://youtu.(iStockphoto#I_UU0xotuXoU_x8y4H3h1+xOg5nY9Ghzd7aII\n"
     ]
    }
   ],
   "source": [
    "prompt = \"write a job posting for sales representative\"\n",
    "generate_text(prompt, max_length=512)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "job_posts_generation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
